{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3db9301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backers_count</th>\n",
       "      <th>category</th>\n",
       "      <th>country</th>\n",
       "      <th>goal</th>\n",
       "      <th>staff_pick</th>\n",
       "      <th>state</th>\n",
       "      <th>usd_pledged</th>\n",
       "      <th>usd_type</th>\n",
       "      <th>deadline_weekday</th>\n",
       "      <th>deadline_month</th>\n",
       "      <th>deadline_year</th>\n",
       "      <th>launched_at_weekday</th>\n",
       "      <th>launched_at_month</th>\n",
       "      <th>launched_at_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>US</td>\n",
       "      <td>120000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>26.00</td>\n",
       "      <td>international</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>150</td>\n",
       "      <td>Literary Spaces</td>\n",
       "      <td>GB</td>\n",
       "      <td>3600</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5028.64</td>\n",
       "      <td>international</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>627</td>\n",
       "      <td>Literary Spaces</td>\n",
       "      <td>US</td>\n",
       "      <td>60000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>77379.85</td>\n",
       "      <td>international</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>365</td>\n",
       "      <td>Literary Spaces</td>\n",
       "      <td>US</td>\n",
       "      <td>50000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>55804.32</td>\n",
       "      <td>international</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22</td>\n",
       "      <td>Literary Spaces</td>\n",
       "      <td>US</td>\n",
       "      <td>1200</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1291.00</td>\n",
       "      <td>international</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    backers_count         category country    goal  staff_pick  state  \\\n",
       "0               2           Comedy      US  120000       False      0   \n",
       "17            150  Literary Spaces      GB    3600       False      1   \n",
       "18            627  Literary Spaces      US   60000        True      1   \n",
       "19            365  Literary Spaces      US   50000        True      1   \n",
       "20             22  Literary Spaces      US    1200       False      1   \n",
       "\n",
       "    usd_pledged       usd_type  deadline_weekday  deadline_month  \\\n",
       "0         26.00  international                 2              12   \n",
       "17      5028.64  international                 2              11   \n",
       "18     77379.85  international                 4              11   \n",
       "19     55804.32  international                 2              11   \n",
       "20      1291.00  international                 0              11   \n",
       "\n",
       "    deadline_year  launched_at_weekday  launched_at_month  launched_at_year  \n",
       "0            2022                    4                 12              2022  \n",
       "17           2022                    6                 10              2022  \n",
       "18           2022                    4                 10              2022  \n",
       "19           2022                    0                 10              2022  \n",
       "20           2022                    5                 10              2022  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Store dataset in a Pandas Dataframe\n",
    "df_ = pd.read_csv(\"preprocessed_Modelisation.csv\", index_col='Unnamed: 0')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#Supprimer les colonnes non pertinentes restantes\n",
    "df_=df_.drop(['state_changed_at_date','state_changed_at_weekday', 'state_changed_at_month','state_changed_at_year','launched_at_date','deadline_date'],axis=1)\n",
    "\n",
    "#remplacer les modalités de la variable cible par 0 et 1\n",
    "df_['state']=df_['state'].replace({'failed':0, 'successful':1})\n",
    "\n",
    "#Aperçu du jeu de données\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7d2bd",
   "metadata": {},
   "source": [
    "# Modèles de classification binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd721218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_cols: ['category', 'country', 'staff_pick', 'usd_type']\n",
      "num_cols: ['backers_count', 'goal', 'usd_pledged', 'deadline_weekday', 'deadline_month', 'deadline_year', 'launched_at_weekday', 'launched_at_month', 'launched_at_year'] \n",
      "\n",
      "Train Set: (2531, 31)\n",
      "Test Set: (633, 31)\n"
     ]
    }
   ],
   "source": [
    "#identification des variables catégorielles et numériques\n",
    "cat_cols = list(df_.drop('state',axis=1).select_dtypes(['object','bool']))\n",
    "num_cols = list(df_.drop('state',axis=1).select_dtypes('number'))\n",
    "print(\"cat_cols:\",cat_cols)\n",
    "print(\"num_cols:\", num_cols,\"\\n\")\n",
    "\n",
    "#encodage des variables catégorielles\n",
    "df1 =pd.get_dummies(df_['staff_pick'], prefix ='staff_pick')\n",
    "df2 =pd.get_dummies(df_['usd_type'],prefix='usd_type')\n",
    "df3 =pd.get_dummies(df_['category'])\n",
    "df4 =pd.get_dummies(df_['country'])\n",
    "\n",
    "df5 = pd.concat([df_,df1,df2,df3,df4],axis=1)\n",
    "df5.drop(['staff_pick','category', 'usd_type', 'country'],axis=1,inplace=True)\n",
    "\n",
    "#Séparer les données\n",
    "X = df5.drop(['state','usd_pledged','backers_count'],axis=1)\n",
    "y = df5['state']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#standardisation des variables numériques\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Train Set:\", X_train.shape)\n",
    "print(\"Test Set:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5efde",
   "metadata": {},
   "source": [
    "### Arbre linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167438d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 0.9996048992493086\n",
      "Score test: 0.8973143759873617 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prédiction</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Realité</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prédiction   0    1\n",
       "Realité            \n",
       "0           63   33\n",
       "1           32  505"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66        96\n",
      "           1       0.94      0.94      0.94       537\n",
      "\n",
      "    accuracy                           0.90       633\n",
      "   macro avg       0.80      0.80      0.80       633\n",
      "weighted avg       0.90      0.90      0.90       633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "#Entraîner le modèle sur le jeu d'entraînement\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "#Faire les prédictions sur le jeu de test avec méthode predict\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "\n",
    "#Évaluer le modèle à l'aide de la méthode score\n",
    "print(\"Score train:\",tree.score(X_train, y_train))\n",
    "print(\"Score test:\",tree.score(X_test, y_test),\"\\n\\n\")\n",
    "\n",
    "#Matrice de confusion et rapport de classification\n",
    "display(pd.crosstab(y_test,y_pred_tree, rownames=['Realité'], colnames=['Prédiction']))\n",
    "print(\"\\n\\n\",classification_report(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0b9a4",
   "metadata": {},
   "source": [
    "### Régression logistique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b375281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 0.9126827340971948\n",
      "Score test: 0.9052132701421801 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prédiction</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Realité</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prédiction   0    1\n",
       "Realité            \n",
       "0           69   27\n",
       "1           33  504"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.72      0.70        96\n",
      "           1       0.95      0.94      0.94       537\n",
      "\n",
      "    accuracy                           0.91       633\n",
      "   macro avg       0.81      0.83      0.82       633\n",
      "weighted avg       0.91      0.91      0.91       633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reg_log = LogisticRegression()\n",
    "\n",
    "#Entraîner le modèle sur le jeu d'entraînement\n",
    "reg_log.fit(X_train, y_train)\n",
    "\n",
    "#Faire les prédictions sur le jeu de test avec méthode predict\n",
    "y_pred_reg_log = reg_log.predict(X_test)\n",
    "\n",
    "#Évaluer le modèle à l'aide de la méthode score\n",
    "print(\"Score train:\",reg_log.score(X_train, y_train))\n",
    "print(\"Score test:\",reg_log.score(X_test, y_test),\"\\n\\n\")\n",
    "\n",
    "#Matrice de confusion et rapport de classification\n",
    "display(pd.crosstab(y_test,y_pred_reg_log, rownames=['Realité'], colnames=['Prédiction']))\n",
    "print(\"\\n\\n\",classification_report(y_test, y_pred_reg_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b48b19",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d158c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 0.9996048992493086\n",
      "Score test: 0.9162717219589257 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prédiction</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Realité</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prédiction   0    1\n",
       "Realité            \n",
       "0           73   23\n",
       "1           30  507"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.73        96\n",
      "           1       0.96      0.94      0.95       537\n",
      "\n",
      "    accuracy                           0.92       633\n",
      "   macro avg       0.83      0.85      0.84       633\n",
      "weighted avg       0.92      0.92      0.92       633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "#Entraîner le modèle sur le jeu d'entraînement\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#Faire les prédictions sur le jeu de test avec méthode predict\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "#Évaluer le modèle à l'aide de la méthode score\n",
    "print('Score train:', rf.score(X_train, y_train))\n",
    "print('Score test:', rf.score(X_test, y_test),\"\\n\\n\")\n",
    "\n",
    "#Matrice de confusion et rapport de classification\n",
    "display(pd.crosstab(y_test,y_pred_rf, rownames=['Realité'], colnames=['Prédiction']))\n",
    "print(\"\\n\\n\",classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e07adaa",
   "metadata": {},
   "source": [
    "### SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6906b279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 0.9996048992493086\n",
      "Score test: 0.9162717219589257 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prédiction</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Realité</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prédiction   0    1\n",
       "Realité            \n",
       "0           75   21\n",
       "1           32  505"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.78      0.74        96\n",
      "           1       0.96      0.94      0.95       537\n",
      "\n",
      "    accuracy                           0.92       633\n",
      "   macro avg       0.83      0.86      0.84       633\n",
      "weighted avg       0.92      0.92      0.92       633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "\n",
    "#Entraîner le modèle sur le jeu d'entraînement\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "#Faire les prédictions sur le jeu de test avec méthode predict\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "\n",
    "#Évaluer le modèle à l'aide de la méthode score\n",
    "print('Score train:', rf.score(X_train, y_train))\n",
    "print('Score test:', rf.score(X_test, y_test),\"\\n\\n\")\n",
    "\n",
    "#Matrice de confusion et rapport de classification\n",
    "display(pd.crosstab(y_test,y_pred_svc, rownames=['Realité'], colnames=['Prédiction']))\n",
    "print(\"\\n\\n\",classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d722e",
   "metadata": {},
   "source": [
    "### XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6930e7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 0.9996048992493086\n",
      "Score test: 0.9146919431279621 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prédiction</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Realité</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prédiction   0    1\n",
       "Realité            \n",
       "0           70   26\n",
       "1           28  509"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72        96\n",
      "           1       0.95      0.95      0.95       537\n",
      "\n",
      "    accuracy                           0.91       633\n",
      "   macro avg       0.83      0.84      0.84       633\n",
      "weighted avg       0.92      0.91      0.92       633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost.compat import XGBClassifierBase\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "#Entraîner le modèle sur le jeu d'entraînement\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "#Faire les prédictions sur le jeu de test avec méthode predict\n",
    "y_pred_xgb =xgb.predict(X_test) \n",
    "\n",
    "#Évaluer le modèle à l'aide de la méthode score\n",
    "print('Score train:', xgb.score(X_train, y_train))\n",
    "print('Score test:', xgb.score(X_test, y_test),\"\\n\\n\")\n",
    "\n",
    "#Matrice de confusion et rapport de classification\n",
    "display(pd.crosstab(y_test,y_pred_xgb, rownames=['Realité'], colnames=['Prédiction']))\n",
    "print(\"\\n\\n\",classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acad000",
   "metadata": {},
   "source": [
    "### AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69fef6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaBoostClassifier\n\u001b[1;32m      2\u001b[0m ada \u001b[38;5;241m=\u001b[39m AdaBoostClassifier ()\n\u001b[0;32m----> 3\u001b[0m ada \u001b[38;5;241m=\u001b[39m AdaBoostClassifier(base_estimator\u001b[38;5;241m=\u001b[39m\u001b[43mtree\u001b[49m, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Entraîner le modèle sur le jeu d'entraînement\u001b[39;00m\n\u001b[1;32m      6\u001b[0m ada\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier ()\n",
    "ada = AdaBoostClassifier(base_estimator=tree, n_estimators=400)\n",
    "\n",
    "#Entraîner le modèle sur le jeu d'entraînement\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "#Faire les prédictions sur le jeu de test avec méthode predict\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "\n",
    "#Évaluer le modèle à l'aide de la méthode score\n",
    "print('Score train:', ada.score(X_train, y_train))\n",
    "print('Score test:', ada.score(X_test, y_test),\"\\n\\n\")\n",
    "\n",
    "#Matrice de confusion et rapport de classification\n",
    "display(pd.crosstab(y_test,y_pred_ada, rownames=['Realité'], colnames=['Prédiction']))\n",
    "print(\"\\n\\n\",classification_report(y_test, y_pred_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff657f",
   "metadata": {},
   "source": [
    "# Comparaison de la performance des modèles testés et sélection du/des 2 meilleur(s)¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c921b",
   "metadata": {},
   "source": [
    "Selon l'accuracy score, nous obtenons 2 modèles dont les performances sont quasi identiques: SVC et RandomForest. En effet leur score sur le jeu de test d'environ 0.88 . Cela signifique qu'ils ont tous les 2 la proportion de prédictions correctes les plus élevées de tous les modèles testés.\n",
    "\n",
    "Ceci dit, le RandomForest a le f1-score pour la classe 1 le plus élevé (0.92) mais le SVC le suit de près avec un f1-score de 0.91.\n",
    "\n",
    "Nous choisissons le modèle le plus performant comme étant le RandomForest même si le SVC l'est tout autant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c5766",
   "metadata": {},
   "source": [
    "# Optimisation du/des modèles sélectionnés "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba6f16",
   "metadata": {},
   "source": [
    "### Avec GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d9f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimisation de RandomForest en utilisant GridSearchCV\n",
    "\n",
    "%%time \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators':[100,150,300],\n",
    "    'criterion':['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth':[5,10,15],\n",
    "}\n",
    "\n",
    "# Initialiser RandomForest\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "# Initialiser GridSearchCV\n",
    "grid = GridSearchCV(rf_clf, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Entraîner le modèle\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273844f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Afficher les paramètres optimaux du modèle RandomForest selon notre GridSearchCV\n",
    "print(\"Les meilleurs paramètres de RandomForest sont:\",grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e0d0f4",
   "metadata": {},
   "source": [
    "### Avec la méthode de réduction des dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appliquer la méthode de réduction des dimensions sur le modèle RandomForest\n",
    "#pour déterminer si la performance du modèle peut être améliorée de cette façon\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "rf_reduit = RandomForestClassifier()\n",
    "\n",
    "#Appliquer la technique de réduction des dimensions au jeu d'entraînement et de test\n",
    "X_train_reduit = pca.fit_transform(X_train)\n",
    "X_test_reduit = pca.transform(X_test)\n",
    "\n",
    "#Entraîner les dimensions réduites sur le modèle le plus performant identifié plus haut\n",
    "rf_reduit.fit(X_train_reduit, y_train)\n",
    "\n",
    "#Faire les prédictions sur le modèle réduit\n",
    "y_pred_reduit = rf_reduit.predict(X_test_reduit)\n",
    "\n",
    "#Évaluer le modèle à l'aide de la méthode score\n",
    "print('Score train:', rf_reduit.score(X_train_reduit, y_train))\n",
    "print('Score test:', rf_reduit.score(X_test_reduit, y_test),\"\\n\\n\")\n",
    "\n",
    "#Matrice de confusion et rapport de classification\n",
    "display(pd.crosstab(y_test,y_pred_reduit, rownames=['Realité'], colnames=['Prédiction']))\n",
    "print(\"\\n\\n\",classification_report(y_test, y_pred_reduit))\n",
    "\n",
    "##Conclusion : en utilisant l'ACP sur RandomForest, le score test est très légèrement \n",
    "#moins bon sur le jeu réduit que sur le jeu non réduit : 0.86 vs 0.88\n",
    "\n",
    "##ce ne sera finalement pas vraiment utile pour notre analyse. De plus, notre jeu de données est sans \n",
    "#doute trop petit pour que cette méthode soit utile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce9e3ff",
   "metadata": {},
   "source": [
    "# Interprétabilité du/des modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857427f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Afficher l'importance de chaque colonne pour le modèle RandomForest rf\n",
    "\n",
    "rf.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04af7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe contenant les variables avec leurs importances\n",
    "\n",
    "columns=['goal', 'deadline_weekday', 'deadline_month', 'deadline_year',\n",
    "       'launched_at_weekday', 'launched_at_month', 'launched_at_year',\n",
    "       'staff_pick_False', 'staff_pick_True', 'usd_type_domestic',\n",
    "       'usd_type_international', 'Audio', 'Comedy', 'Cookbooks', 'Design',\n",
    "       'Graphic Novels', 'Literary Journals', 'Literary Spaces', 'Photo',\n",
    "       'Plays', 'R&B', 'Sound', 'Spaces', 'Toys', 'Webcomics', 'Asia', 'CA',\n",
    "       'Europe', 'GB', 'MX', 'Oceania', 'US']\n",
    "\n",
    "X_features=list(rf.feature_importances_)\n",
    "\n",
    "df_feature=pd.DataFrame({'Feature':columns,\n",
    "                         'Score':X_features})\n",
    "\n",
    "df_feature=df_feature.sort_values(['Score'], ascending=[False])\n",
    "\n",
    "df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66657a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additionner les importances des sous-colonnes pour les regrouper en 6 variables\n",
    "\n",
    "L=[]\n",
    "imp_deadline = 0\n",
    "imp_launched = 0\n",
    "imp_staff = 0\n",
    "imp_usdt = 0\n",
    "imp_categ = 0\n",
    "imp_geo = 0 \n",
    "\n",
    "if i in range(1,31):\n",
    "  for i in [1,2,3]:\n",
    "    imp_deadline += rf.feature_importances_[i]\n",
    "  for i in [4,5,6]:\n",
    "    imp_launched += rf.feature_importances_[i]\n",
    "  for i in [7,8]:\n",
    "    imp_staff += rf.feature_importances_[i]\n",
    "  for i in [9,10]:\n",
    "    imp_usdt += rf.feature_importances_[i]\n",
    "  for i in range(11, 25):\n",
    "    imp_categ += rf.feature_importances_[i]\n",
    "  for i in range(25, 31):\n",
    "    imp_geo += rf.feature_importances_[i]\n",
    "\n",
    "L.append([imp_deadline, imp_launched, imp_staff,imp_usdt,imp_categ,imp_geo])\n",
    "\n",
    "\n",
    "importances = [rf.feature_importances_[0],imp_deadline, imp_launched, imp_staff,imp_usdt,imp_categ,imp_geo]\n",
    "variables = ['goal', 'deadline', 'launched_at', 'staff', 'usd_type', 'category', 'zone_geo']\n",
    "\n",
    "df_importances = pd.DataFrame({'Variable':variables, 'Importance':importances}).sort_values(['Importance'], ascending=[False])\n",
    "df_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9769bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faire un barplot de la somme des importances des variables\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "fig, ax = plt.subplots()\n",
    "ax= plt.bar(variables,importances, width=0.7)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Importances des variables',fontsize=14)\n",
    "plt.xlabel('Variables',fontsize=12, labelpad=10)\n",
    "plt.ylabel('Importance',fontsize=12, labelpad=10)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparer y_test (données réélles) aux données prédites par le modèle RandomForest (y_pred_rf)\n",
    "predictions = pd.DataFrame({'y_test': y_test, 'y_pred_rf': y_pred_rf})\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9bb60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Déterminer sur quelles campagnes le modèle RandomForest a fait des erreurs\n",
    "\n",
    "indices_erreur = y_pred_rf != y_test\n",
    "X_test_erreur = X_test[indices_erreur]\n",
    "erreurs = pd.DataFrame(X_test_erreur['goal']).T\n",
    "\n",
    "X_test_erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffb3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#étudier la répartition des valeurs de \"goal\" pour identifier si cette variable peut être la cause des erreurs\n",
    "X_test_erreur['goal'].describe()\n",
    "\n",
    "#en regardant la valeur de \"goal\" pour chaque ligne surlaquelle le modèle s'est trompé,\n",
    "#on remarque que la valeur est soit très faible soit très élevée: l'écart type est très\n",
    "#grand ce qui signifie une grande variabilité dans les données. De plus la médiane\n",
    "#est très largement inférieure à la moyenne ce qui peut suggérer des valeurs extrêmes.\n",
    "#on peut en conclure que les erreurs sont faites sur les valeurs extrêmes de \"goal\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
